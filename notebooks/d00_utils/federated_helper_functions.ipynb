{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec0bdf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "#Pandas: Reading and analyzing data\n",
    "import pandas as pd\n",
    "#Numerical calcuations\n",
    "import numpy as np\n",
    "#Evaluate models\n",
    "import math\n",
    "\n",
    "#Keras: Open-Source deep-learning library \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "\n",
    "from model_helper_functions import *\n",
    "from windowgenerator import *\n",
    "\n",
    "def weight_scalling_factor(clients_trn_data, client_name, client_names):\n",
    "    \"\"\"\n",
    "    weight_scalling_factor calculates the proportion of a client’s local training data \n",
    "    with the overall training data held by all clients. First the client’s batch size is obtained and used \n",
    "    to calculate its number of data points.Then the overall global training data size is obtained (global_count).\n",
    "    Finally we calculated the scaling factor as a fraction (return). \n",
    "    \"\"\"\n",
    "    #get the bs\n",
    "    bs = list(clients_trn_data)[0][0].shape[0]\n",
    "    #first calculate the total training data points across clinets\n",
    "    global_count = sum([tf.data.experimental.cardinality(clients_trn_data).numpy() for client_name in client_names])*bs\n",
    "    # get the total number of data points held by a client\n",
    "    local_count = tf.data.experimental.cardinality(clients_trn_data).numpy()*bs\n",
    "    return local_count/global_count\n",
    "\n",
    "def scale_model_weights(weight, scalar):\n",
    "    \"\"\"\n",
    "    scale_model_weights scales each of the local model’s weights based the value of their scaling factor calculated in weight_scalling_factor\n",
    "    \"\"\"\n",
    "    weight_final = []\n",
    "    steps = len(weight)\n",
    "    for i in range(steps):\n",
    "        weight_final.append(scalar * weight[i])\n",
    "    return weight_final\n",
    "\n",
    "def sum_scaled_weights(scaled_weight_list):\n",
    "    \"\"\"\n",
    "    Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights\n",
    "    \"\"\"\n",
    "    avg_grad = list()\n",
    "    #get the average grad accross all client gradients\n",
    "    for grad_list_tuple in zip(*scaled_weight_list):\n",
    "        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n",
    "        avg_grad.append(layer_mean)\n",
    "        \n",
    "    return avg_grad\n",
    "\n",
    "       \n",
    "def createGlobalModelsForClusters(\n",
    "        windows_dict, INPUT_SHAPE, OUT_STEPS, NUM_FEATURES, \n",
    "        NUM_LSTM_CELLS, NUM_LSTM_LAYERS, NUM_LSTM_DENSE_LAYERS, NUM_LSTM_DENSE_UNITS, LSTM_DROPOUT, \n",
    "        CONV_WIDTH, NUM_CNN_LAYERS, NUM_CNN_FILTERS, NUM_CNN_DENSE_LAYERS, NUM_CNN_DENSE_UNITS, CNN_DROPOUT, \n",
    "    ):\n",
    "    \"\"\"\n",
    "    Create a global LSTM, CNN, and Transofrmer model for each of the clusters\n",
    "    \n",
    "    :param: architecture parameters of the models\n",
    "    :return: 3 arrays with number of clusters LSTM, CNN, and Transofrmer models\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Features 5, Horizon 12\n",
    "    global_LSTM_models = []\n",
    "    global_CNN_models = []\n",
    "    global_Transformer_models = []\n",
    "\n",
    "    for idx, cluster in enumerate(windows_dict):\n",
    "\n",
    "        #Build Models\n",
    "        global_LSTM_models.append(LSTM_Model().build(\n",
    "            input_shape = INPUT_SHAPE[0], \n",
    "            num_LSTM_cells = NUM_LSTM_CELLS,\n",
    "            num_LSTM_layers = NUM_LSTM_LAYERS,\n",
    "            num_LSTM_dense_layers = NUM_LSTM_DENSE_LAYERS,\n",
    "            num_LSTM_dense_units = NUM_LSTM_DENSE_UNITS,\n",
    "            LSTM_dropout = LSTM_DROPOUT,\n",
    "            output_steps = OUT_STEPS[0],\n",
    "            num_features = NUM_FEATURES[0],\n",
    "            model_name = 'Federated_LSTM_F5_H12'\n",
    "        ))\n",
    "        #CNN        \n",
    "        global_CNN_models.append(CNN_Model().build(\n",
    "            input_shape = INPUT_SHAPE[0], \n",
    "            conv_width = CONV_WIDTH,\n",
    "            num_CNN_layers = NUM_CNN_LAYERS,\n",
    "            num_CNN_filters = NUM_CNN_FILTERS,\n",
    "            num_CNN_dense_layers = NUM_CNN_DENSE_LAYERS,\n",
    "            num_CNN_dense_units = NUM_CNN_DENSE_UNITS,\n",
    "            CNN_dropout = CNN_DROPOUT,\n",
    "            output_steps = OUT_STEPS[0],\n",
    "            num_features = NUM_FEATURES[0],\n",
    "            model_name = 'Federated_CNN_F5_H12'\n",
    "        ))\n",
    "        #Transformer\n",
    "        global_Transformer_models.append(Transformer_Model().build(\n",
    "            input_shape = INPUT_SHAPE[0],\n",
    "            output_steps = OUT_STEPS[0],\n",
    "            num_features = NUM_FEATURES[0],\n",
    "            model_name = 'Federated_Transformer_F5_H12'    \n",
    "        ))\n",
    "         \n",
    "    return global_LSTM_models, global_CNN_models, global_Transformer_models\n",
    "    \n",
    "    \n",
    "def getClientNamesOfCluster(windows_dict, cluster):\n",
    "    \"\"\"\n",
    "    Get a list of all clients within the current cluster \n",
    "    \n",
    "    :param windows_dict: dictionary with data windos sorted by cluster\n",
    "    :return: list of client names within current cluster\n",
    "    \"\"\"\n",
    "    \n",
    "    #Get names of clients within cluster\n",
    "    client_names = list()\n",
    "    for client in windows_dict[cluster]:\n",
    "        client_names.append(client)\n",
    "        \n",
    "    return client_names\n",
    "\n",
    "\n",
    "def compile_fit_set_weights(local_model, global_weights, window, client, client_names, MAX_EPOCHS):\n",
    "    \"\"\"\n",
    "    Takes a model, compiles it, sets global weights, fits the model and retunrs new weights\n",
    "    \n",
    "    :param: model, global weights, the window to train and validate with\n",
    "    :return: array of sclaed weights\n",
    "    \"\"\"\n",
    "    #Compile Model (define loss, optimizer, metrics)\n",
    "    local_model.compile(\n",
    "        loss=tf.keras.losses.MeanSquaredError(),\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        metrics=[tf.keras.metrics.RootMeanSquaredError(), \n",
    "            tf.keras.metrics.MeanAbsolutePercentageError(),\n",
    "            tf.keras.metrics.MeanAbsoluteError(),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    #set local model weight to the weight of the global model\n",
    "    local_model.set_weights(global_weights)\n",
    "    \n",
    "    #fit local model with client's data\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=2,mode='min')\n",
    "    local_model.fit(\n",
    "        window.train, \n",
    "        epochs=MAX_EPOCHS, \n",
    "        verbose=1, \n",
    "        validation_data=window.val,\n",
    "        callbacks=[\n",
    "            timetaken,\n",
    "            #early_stopping, \n",
    "            #create_model_checkpoint(save_path=save_path), \n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    #scale the model weights and add to list        \n",
    "    scaling_factor = weight_scalling_factor(window.train, client, client_names)\n",
    "    scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
    "    \n",
    "    return scaled_weights\n",
    "    \n",
    "\n",
    "def loadGlobalModels( cwd, global_LSTM_models, global_CNN_models, global_Transformer_models, idx, idx_com\n",
    "    ):\n",
    "    #load global model of last federated round if not first round\n",
    "    if idx_com != 0:\n",
    "        idx_com = idx_com-1\n",
    "        \n",
    "    #load model\n",
    "    global_LSTM_model = keras.models.load_model(cwd + f\"/data/d05_models/cluster{idx}/{global_LSTM_models[idx].name}/FederatedRound{idx_com}\", compile=False)\n",
    "    global_CNN_model = keras.models.load_model(cwd + f\"/data/d05_models/cluster{idx}/{global_CNN_models[idx].name}/FederatedRound{idx_com}\", compile=False)\n",
    "    global_Transformer_model = keras.models.load_model(cwd + f\"/data/d05_models/cluster{idx}/{global_Transformer_models[idx].name}/FederatedRound{idx_com}\", compile=False)\n",
    "    \n",
    "    return global_LSTM_model, global_CNN_model, global_Transformer_model\n",
    "    \n",
    "def getGlobalModelWeights(\n",
    "        global_LSTM_model, global_CNN_model, global_Transformer_model\n",
    "    ):\n",
    "          \n",
    "    #Get model weights\n",
    "    global_LSTM_weights = global_LSTM_model.get_weights()\n",
    "    global_CNN_weights = global_CNN_model.get_weights()\n",
    "    global_Transformer_weights = global_Transformer_model.get_weights()\n",
    "                                                       \n",
    "    return global_LSTM_weights, global_CNN_weights, global_Transformer_weights\n",
    "\n",
    "def initiallySaveAllGlobalModels(cwd, global_LSTM_models, global_CNN_models, global_Transformer_models):\n",
    "    for cluster_idx in range(6):\n",
    "        # LSTM\n",
    "        global_LSTM_models[cluster_idx].save(cwd + f\"/data/d05_models/cluster{cluster_idx}/{global_LSTM_models[cluster_idx].name}/FederatedRound{0}\")\n",
    "        #Cnn\n",
    "        global_CNN_models[cluster_idx].save(cwd + f\"/data/d05_models/cluster{cluster_idx}/{global_CNN_models[cluster_idx].name}/FederatedRound{0}\")\n",
    "        #Transformer\n",
    "        global_Transformer_models[cluster_idx].save(cwd + f\"/data/d05_models/cluster{cluster_idx}/{global_Transformer_models[cluster_idx].name}/FederatedRound{0}\")\n",
    "\n",
    "def saveGlobalModels(cwd, global_LSTM_model, global_CNN_model, global_Transformer_model, idx, idx_com):\n",
    "    \"\"\"\n",
    "    Save the global models  \n",
    "    \n",
    "    :param: global models, cluster idx und federated round idx\n",
    "    \"\"\"\n",
    "    # LSTM\n",
    "    global_LSTM_model.save(cwd + f\"/data/d05_models/cluster{idx}/{global_LSTM_model.name}/FederatedRound{idx_com}\")\n",
    "    #Cnn\n",
    "    global_CNN_model.save(cwd + f\"/data/d05_models/cluster{idx}/{global_CNN_model.name}/FederatedRound{idx_com}\")\n",
    "    #Transformer\n",
    "    global_Transformer_model.save(cwd + f\"/data/d05_models/cluster{idx}/{global_Transformer_model.name}/FederatedRound{idx_com}\")\n",
    "\n",
    "def loadCompileEvaluateModel(path, window):\n",
    "                \n",
    "        model = tf.keras.models.load_model(path)\n",
    "        model.compile(\n",
    "            loss=tf.keras.losses.MeanSquaredError(),\n",
    "            optimizer=tf.keras.optimizers.Adam(),\n",
    "            metrics=[tf.keras.metrics.RootMeanSquaredError(), \n",
    "                tf.keras.metrics.MeanAbsolutePercentageError(),\n",
    "                tf.keras.metrics.MeanAbsoluteError(),\n",
    "            ]\n",
    "        )\n",
    "        model_evaluation_test = model.evaluate(window.test)\n",
    "        return model_evaluation_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
