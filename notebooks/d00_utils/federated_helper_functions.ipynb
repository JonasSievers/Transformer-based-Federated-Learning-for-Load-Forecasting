{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec0bdf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "#Pandas: Reading and analyzing data\n",
    "import pandas as pd\n",
    "#Numerical calcuations\n",
    "import numpy as np\n",
    "#Evaluate models\n",
    "import math\n",
    "\n",
    "#Keras: Open-Source deep-learning library \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "\n",
    "from model_helper_functions import *\n",
    "from windowgenerator import *\n",
    "\n",
    "def weight_scalling_factor(clients_trn_data, client_name, client_names):\n",
    "    \"\"\"\n",
    "    weight_scalling_factor calculates the proportion of a client’s local training data \n",
    "    with the overall training data held by all clients. First the client’s batch size is obtained and used \n",
    "    to calculate its number of data points.Then the overall global training data size is obtained (global_count).\n",
    "    Finally we calculated the scaling factor as a fraction (return). \n",
    "    \"\"\"\n",
    "    #get the bs\n",
    "    bs = list(clients_trn_data)[0][0].shape[0]\n",
    "    #first calculate the total training data points across clinets\n",
    "    global_count = sum([tf.data.experimental.cardinality(clients_trn_data).numpy() for client_name in client_names])*bs\n",
    "    # get the total number of data points held by a client\n",
    "    local_count = tf.data.experimental.cardinality(clients_trn_data).numpy()*bs\n",
    "    return local_count/global_count\n",
    "\n",
    "def scale_model_weights(weight, scalar):\n",
    "    \"\"\"\n",
    "    scale_model_weights scales each of the local model’s weights based the value of their scaling factor calculated in weight_scalling_factor\n",
    "    \"\"\"\n",
    "    weight_final = []\n",
    "    steps = len(weight)\n",
    "    for i in range(steps):\n",
    "        weight_final.append(scalar * weight[i])\n",
    "    return weight_final\n",
    "\n",
    "def sum_scaled_weights(scaled_weight_list):\n",
    "    \"\"\"\n",
    "    Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights\n",
    "    \"\"\"\n",
    "    avg_grad = list()\n",
    "    #get the average grad accross all client gradients\n",
    "    for grad_list_tuple in zip(*scaled_weight_list):\n",
    "        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n",
    "        avg_grad.append(layer_mean)\n",
    "        \n",
    "    return avg_grad\n",
    "\n",
    "def createDataWindows(y, smart_meter_names, INPUT_STEPS, OUT_STEPS, ds_dict, N_CLUSTERS): \n",
    "    \"\"\"\n",
    "    Create a window for every client considering each horizon (12, 24) and each featureset (5,7)\n",
    "    \n",
    "    :param smart_meter_names: names of clients\n",
    "    :return: dictionary with structure windows_dict[0-nr_clusters][client_i_smart_meter_names][0-3] \n",
    "        -> 0:window_F5_H12 , 1:window_F5_H24 , 2:window_F7_H12 , 3:window_F7_H24\n",
    "    \"\"\"\n",
    "    \n",
    "    windows_dict = {k: {} for k in range(N_CLUSTERS)}\n",
    "    \n",
    "    for i, client in enumerate(smart_meter_names):\n",
    "        \n",
    "        #window_F5_H12\n",
    "        window_F5_H12 = WindowGenerator(\n",
    "            input_width=INPUT_STEPS, label_width=OUT_STEPS[0], shift=OUT_STEPS[0], \n",
    "            train_df = ds_dict[client][3], val_df = ds_dict[client][4], test_df = ds_dict[client][5], label_columns=[client]\n",
    "        )\n",
    "        example_window = tf.stack([np.array(ds_dict[client][3][10100:10100+window_F5_H12.total_window_size]),\n",
    "                                   np.array(ds_dict[client][3][2000:2000+window_F5_H12.total_window_size]),\n",
    "                                   np.array(ds_dict[client][3][3000:3000+window_F5_H12.total_window_size])])\n",
    "        example_inputs, example_labels = window_F5_H12.split_window(example_window)\n",
    "        window_F5_H12.example = example_inputs, example_labels\n",
    "\n",
    "        #window_F5_H24\n",
    "        window_F5_H24 = WindowGenerator(\n",
    "            input_width=INPUT_STEPS, label_width=OUT_STEPS[1], shift=OUT_STEPS[1], \n",
    "            train_df = ds_dict[client][3], val_df = ds_dict[client][4], test_df = ds_dict[client][5], label_columns=[client]\n",
    "        )\n",
    "        example_window = tf.stack([np.array(ds_dict[client][3][10100:10100+window_F5_H24.total_window_size]),\n",
    "                                   np.array(ds_dict[client][3][2000:2000+window_F5_H24.total_window_size]),\n",
    "                                   np.array(ds_dict[client][3][3000:3000+window_F5_H24.total_window_size])])\n",
    "        example_inputs, example_labels = window_F5_H24.split_window(example_window)\n",
    "        window_F5_H24.example = example_inputs, example_labels\n",
    "\n",
    "        #window_F7_H12\n",
    "        window_F7_H12 = WindowGenerator(\n",
    "            input_width=INPUT_STEPS, label_width=OUT_STEPS[0], shift=OUT_STEPS[0], \n",
    "            train_df = ds_dict[client][0], val_df = ds_dict[client][1], test_df = ds_dict[client][2], label_columns=[client]\n",
    "        )\n",
    "        example_window = tf.stack([np.array(ds_dict[client][0][10100:10100+window_F7_H12.total_window_size]),\n",
    "                                   np.array(ds_dict[client][0][2000:2000+window_F7_H12.total_window_size]),\n",
    "                                   np.array(ds_dict[client][0][3000:3000+window_F7_H12.total_window_size])])\n",
    "        example_inputs, example_labels = window_F7_H12.split_window(example_window)\n",
    "        window_F7_H12.example = example_inputs, example_labels\n",
    "\n",
    "        #window_F5_H24\n",
    "        window_F7_H24 = WindowGenerator(\n",
    "            input_width=INPUT_STEPS, label_width=OUT_STEPS[1], shift=OUT_STEPS[1], \n",
    "            train_df = ds_dict[client][0], val_df = ds_dict[client][1], test_df = ds_dict[client][2], label_columns=[client]\n",
    "        )\n",
    "        example_window = tf.stack([np.array(ds_dict[client][0][10100:10100+window_F7_H24.total_window_size]),\n",
    "                                   np.array(ds_dict[client][0][2000:2000+window_F7_H24.total_window_size]),\n",
    "                                   np.array(ds_dict[client][0][3000:3000+window_F7_H24.total_window_size])])\n",
    "        example_inputs, example_labels = window_F7_H24.split_window(example_window)\n",
    "        window_F7_H24.example = example_inputs, example_labels\n",
    "\n",
    "        windows_dict[y[i]]['{}_{}_{}'.format('client', i+1, client)] = [window_F5_H12, window_F5_H24, window_F7_H12, window_F7_H24]\n",
    "    \n",
    "    return windows_dict\n",
    "\n",
    "def makeDatasetsForclientsAndfeatures(smart_meter_names, df):\n",
    "    \"\"\"\n",
    "    Create datasets (train, val, test) for every client considering each featureset (5,7)\n",
    "    \n",
    "    :param smart_meter_names: names of clients\n",
    "    :param df: dataframe\n",
    "    :return: dictionary with structure ds_dict[smart_meter_names][0-5] \n",
    "       -> 0:train_df_F7, 1: val_df_F7, 2: test_df_F7, 3: train_df_F5, 4: val_df_F5, 5: test_df_F5\n",
    "    \n",
    "    \"\"\"\n",
    "    ds_dict = {}\n",
    "    n = len(df)\n",
    "    for client in smart_meter_names:   \n",
    "        train_df_F7 = df[0:int(n*0.7)][[client, 'temp', 'rhum', 'hour sin', 'hour cos', 'dayofweek sin', 'dayofweek cos']]\n",
    "        val_df_F7 = df[int(n*0.7):int(n*0.9)][[client, 'temp', 'rhum', 'hour sin', 'hour cos', 'dayofweek sin', 'dayofweek cos']]\n",
    "        test_df_F7 = df[int(n*0.9):][[client, 'temp', 'rhum', 'hour sin', 'hour cos', 'dayofweek sin', 'dayofweek cos']]\n",
    "\n",
    "        train_df_F5 = df[0:int(n*0.7)][[client, 'hour sin', 'hour cos', 'dayofweek sin', 'dayofweek cos']]\n",
    "        val_df_F5 = df[int(n*0.7):int(n*0.9)][[client, 'hour sin', 'hour cos', 'dayofweek sin', 'dayofweek cos']]\n",
    "        test_df_F5 = df[int(n*0.9):][[client, 'hour sin', 'hour cos', 'dayofweek sin', 'dayofweek cos']]\n",
    "\n",
    "        ds_dict[client] = [train_df_F7, val_df_F7, test_df_F7, train_df_F5, val_df_F5, test_df_F5]\n",
    "    \n",
    "    return ds_dict\n",
    "\n",
    "def InititalizeResultDictionary(learning_style=\"Federated\"):\n",
    "    \"\"\"\n",
    "    Iniitalize dictionary to save training results\n",
    "    \n",
    "    :param learning_style: style of learning - federated, centralized, local\n",
    "    :return: dictionary with structure ds_dict[learning_style][model][horizon][features]     \n",
    "    \"\"\"\n",
    "    #Initialize results\n",
    "    final_dict = {}\n",
    "    final_dict[learning_style] = {}\n",
    "    final_dict[learning_style]['LSTM'] = {}\n",
    "    final_dict[learning_style]['LSTM']['H12'] = {}\n",
    "    final_dict[learning_style]['LSTM']['H12']['F5'] = {}\n",
    "    final_dict[learning_style]['LSTM']['H12']['F7'] = {}\n",
    "    #----------------------------------------------\n",
    "    final_dict[learning_style]['LSTM']['H24'] = {}\n",
    "    final_dict[learning_style]['LSTM']['H24']['F5'] = {}\n",
    "    final_dict[learning_style]['LSTM']['H24']['F7'] = {}\n",
    "\n",
    "    final_dict[learning_style]['CNN'] = {}\n",
    "    final_dict[learning_style]['CNN']['H12'] = {}\n",
    "    final_dict[learning_style]['CNN']['H12']['F5'] = {}\n",
    "    final_dict[learning_style]['CNN']['H12']['F7'] = {}\n",
    "    #----------------------------------------------\n",
    "    final_dict[learning_style]['CNN']['H24'] = {}\n",
    "    final_dict[learning_style]['CNN']['H24']['F5'] = {}\n",
    "    final_dict[learning_style]['CNN']['H24']['F7'] = {}\n",
    "\n",
    "    final_dict[learning_style]['Transformer'] = {}\n",
    "    final_dict[learning_style]['Transformer']['H12'] = {}\n",
    "    final_dict[learning_style]['Transformer']['H12']['F5'] = {}\n",
    "    final_dict[learning_style]['Transformer']['H12']['F7'] = {}\n",
    "    #----------------------------------------------\n",
    "    final_dict[learning_style]['Transformer']['H24'] = {}\n",
    "    final_dict[learning_style]['Transformer']['H24']['F5'] = {}\n",
    "    final_dict[learning_style]['Transformer']['H24']['F7'] = {}\n",
    "    \n",
    "    return final_dict\n",
    "    \n",
    "    \n",
    "def createGlobalModelsForClusters(\n",
    "        windows_dict, INPUT_SHAPE, OUT_STEPS, NUM_FEATURES, \n",
    "        NUM_LSTM_CELLS, NUM_LSTM_LAYERS, NUM_LSTM_DENSE_LAYERS, NUM_LSTM_DENSE_UNITS, LSTM_DROPOUT, \n",
    "        CONV_WIDTH, NUM_CNN_LAYERS, NUM_CNN_FILTERS, NUM_CNN_DENSE_LAYERS, NUM_CNN_DENSE_UNITS, CNN_DROPOUT, \n",
    "    ):\n",
    "    \"\"\"\n",
    "    Create a global LSTM, CNN, and Transofrmer model for each of the clusters\n",
    "    \n",
    "    :param: architecture parameters of the models\n",
    "    :return: 3 arrays with number of clusters LSTM, CNN, and Transofrmer models\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Features 5, Horizon 12\n",
    "    global_LSTM_models = []\n",
    "    global_CNN_models = []\n",
    "    global_Transformer_models = []\n",
    "\n",
    "    for idx, cluster in enumerate(windows_dict):\n",
    "\n",
    "        #Build Models\n",
    "        global_LSTM_models.append(LSTM_Model().build(\n",
    "            input_shape = INPUT_SHAPE[0], \n",
    "            num_LSTM_cells = NUM_LSTM_CELLS,\n",
    "            num_LSTM_layers = NUM_LSTM_LAYERS,\n",
    "            num_LSTM_dense_layers = NUM_LSTM_DENSE_LAYERS,\n",
    "            num_LSTM_dense_units = NUM_LSTM_DENSE_UNITS,\n",
    "            LSTM_dropout = LSTM_DROPOUT,\n",
    "            output_steps = OUT_STEPS[0],\n",
    "            num_features = NUM_FEATURES[0],\n",
    "            model_name = 'Federated_LSTM_F5_H12'\n",
    "        ))\n",
    "        #CNN        \n",
    "        global_CNN_models.append(CNN_Model().build(\n",
    "            input_shape = INPUT_SHAPE[0], \n",
    "            conv_width = CONV_WIDTH,\n",
    "            num_CNN_layers = NUM_CNN_LAYERS,\n",
    "            num_CNN_filters = NUM_CNN_FILTERS,\n",
    "            num_CNN_dense_layers = NUM_CNN_DENSE_LAYERS,\n",
    "            num_CNN_dense_units = NUM_CNN_DENSE_UNITS,\n",
    "            CNN_dropout = CNN_DROPOUT,\n",
    "            output_steps = OUT_STEPS[0],\n",
    "            num_features = NUM_FEATURES[0],\n",
    "            model_name = 'Federated_CNN_F5_H12'\n",
    "        ))\n",
    "        #Transformer\n",
    "        global_Transformer_models.append(Transformer_Model().build(\n",
    "            input_shape = INPUT_SHAPE[0],\n",
    "            output_steps = OUT_STEPS[0],\n",
    "            num_features = NUM_FEATURES[0],\n",
    "            model_name = 'Federated_Transformer_F5_H12'    \n",
    "        ))\n",
    "         \n",
    "    return global_LSTM_models, global_CNN_models, global_Transformer_models\n",
    "    \n",
    "    \n",
    "def getClientNamesOfCluster(windows_dict, cluster):\n",
    "    \"\"\"\n",
    "    Get a list of all clients within the current cluster \n",
    "    \n",
    "    :param windows_dict: dictionary with data windos sorted by cluster\n",
    "    :return: list of client names within current cluster\n",
    "    \"\"\"\n",
    "    \n",
    "    #Get names of clients within cluster\n",
    "    client_names = list()\n",
    "    for client in windows_dict[cluster]:\n",
    "        client_names.append(client)\n",
    "        \n",
    "    return client_names\n",
    "\n",
    "\n",
    "def compile_fit_set_weights(local_model, global_weights, window, client, client_names, MAX_EPOCHS):\n",
    "    \"\"\"\n",
    "    Takes a model, compiles it, sets global weights, fits the model and retunrs new weights\n",
    "    \n",
    "    :param: model, global weights, the window to train and validate with\n",
    "    :return: array of sclaed weights\n",
    "    \"\"\"\n",
    "    #Compile Model (define loss, optimizer, metrics)\n",
    "    local_model.compile(\n",
    "        loss=tf.keras.losses.MeanSquaredError(),\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        metrics=[tf.keras.metrics.RootMeanSquaredError(), \n",
    "            tf.keras.metrics.MeanAbsolutePercentageError(),\n",
    "            tf.keras.metrics.MeanAbsoluteError(),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    #set local model weight to the weight of the global model\n",
    "    local_model.set_weights(global_weights)\n",
    "    \n",
    "    #fit local model with client's data\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=2,mode='min')\n",
    "    local_model.fit(\n",
    "        window.train, \n",
    "        epochs=MAX_EPOCHS, \n",
    "        verbose=1, \n",
    "        validation_data=window.val,\n",
    "        callbacks=[\n",
    "            timetaken,\n",
    "            #early_stopping, \n",
    "            #create_model_checkpoint(save_path=save_path), \n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    #scale the model weights and add to list        \n",
    "    scaling_factor = weight_scalling_factor(window.train, client, client_names)\n",
    "    scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
    "    \n",
    "    return scaled_weights\n",
    "    \n",
    "\n",
    "def loadGlobalModels( cwd, global_LSTM_models, global_CNN_models, global_Transformer_models, idx, idx_com\n",
    "    ):\n",
    "    #load global model of last federated round if not first round\n",
    "    if idx_com != 0:\n",
    "        idx_com = idx_com-1\n",
    "        \n",
    "    #load model\n",
    "    global_LSTM_model = keras.models.load_model(cwd + f\"/data/d05_models/cluster{idx}/{global_LSTM_models[idx].name}/FederatedRound{idx_com}\", compile=False)\n",
    "    global_CNN_model = keras.models.load_model(cwd + f\"/data/d05_models/cluster{idx}/{global_CNN_models[idx].name}/FederatedRound{idx_com}\", compile=False)\n",
    "    global_Transformer_model = keras.models.load_model(cwd + f\"/data/d05_models/cluster{idx}/{global_Transformer_models[idx].name}/FederatedRound{idx_com}\", compile=False)\n",
    "    \n",
    "    return global_LSTM_model, global_CNN_model, global_Transformer_model\n",
    "    \n",
    "def getGlobalModelWeights(\n",
    "        global_LSTM_model, global_CNN_model, global_Transformer_model\n",
    "    ):\n",
    "          \n",
    "    #Get model weights\n",
    "    global_LSTM_weights = global_LSTM_model.get_weights()\n",
    "    global_CNN_weights = global_CNN_model.get_weights()\n",
    "    global_Transformer_weights = global_Transformer_model.get_weights()\n",
    "                                                       \n",
    "    return global_LSTM_weights, global_CNN_weights, global_Transformer_weights\n",
    "\n",
    "def initiallySaveAllGlobalModels(cwd, global_LSTM_models, global_CNN_models, global_Transformer_models):\n",
    "    for cluster_idx in range(6):\n",
    "        # LSTM\n",
    "        global_LSTM_models[cluster_idx].save(cwd + f\"/data/d05_models/cluster{cluster_idx}/{global_LSTM_models[cluster_idx].name}/FederatedRound{0}\")\n",
    "        #Cnn\n",
    "        global_CNN_models[cluster_idx].save(cwd + f\"/data/d05_models/cluster{cluster_idx}/{global_CNN_models[cluster_idx].name}/FederatedRound{0}\")\n",
    "        #Transformer\n",
    "        global_Transformer_models[cluster_idx].save(cwd + f\"/data/d05_models/cluster{cluster_idx}/{global_Transformer_models[cluster_idx].name}/FederatedRound{0}\")\n",
    "\n",
    "def saveGlobalModels(cwd, global_LSTM_model, global_CNN_model, global_Transformer_model, idx, idx_com):\n",
    "    \"\"\"\n",
    "    Save the global models  \n",
    "    \n",
    "    :param: global models, cluster idx und federated round idx\n",
    "    \"\"\"\n",
    "    # LSTM\n",
    "    global_LSTM_model.save(cwd + f\"/data/d05_models/cluster{idx}/{global_LSTM_model.name}/FederatedRound{idx_com}\")\n",
    "    #Cnn\n",
    "    global_CNN_model.save(cwd + f\"/data/d05_models/cluster{idx}/{global_CNN_model.name}/FederatedRound{idx_com}\")\n",
    "    #Transformer\n",
    "    global_Transformer_model.save(cwd + f\"/data/d05_models/cluster{idx}/{global_Transformer_model.name}/FederatedRound{idx_com}\")\n",
    "\n",
    "def loadCompileEvaluateModel(path, window):\n",
    "                \n",
    "        model = tf.keras.models.load_model(path)\n",
    "        model.compile(\n",
    "            loss=tf.keras.losses.MeanSquaredError(),\n",
    "            optimizer=tf.keras.optimizers.Adam(),\n",
    "            metrics=[tf.keras.metrics.RootMeanSquaredError(), \n",
    "                tf.keras.metrics.MeanAbsolutePercentageError(),\n",
    "                tf.keras.metrics.MeanAbsoluteError(),\n",
    "            ]\n",
    "        )\n",
    "        model_evaluation_test = model.evaluate(window.test)\n",
    "        return model_evaluation_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
