{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6479af5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "\n",
    "# pandas for reading and analyzing data\n",
    "import pandas as pd\n",
    "# numpy for numerical calcuations\n",
    "import numpy as np\n",
    "# seaborn for statistical data visualization\n",
    "import seaborn as sns\n",
    "# datetime to use dates in datetime format\n",
    "import datetime\n",
    "# math to calculate model evaluation steps\n",
    "import math\n",
    "# sklearn for minMaxSclaing and mse\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# matplotlib to plot numpy array\n",
    "import matplotlib.pyplot as plt\n",
    "#tslearn for K-Means Clustering\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "# os to find path of files \n",
    "import os\n",
    "\n",
    "# tensorflow as machine learning library\n",
    "import tensorflow as tf\n",
    "# keras as open-source deep-learning library \n",
    "from tensorflow import keras\n",
    "# building blocks of NN in Keras\n",
    "from tensorflow.keras import layers\n",
    "# earlyStop to stop training early\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# IPython to Clear terminal output\n",
    "import IPython\n",
    "import IPython.display\n",
    "# time and timeit to provie a callback to logg model fitting time\n",
    "import time\n",
    "from timeit import default_timer as timer\n",
    "# logging to logg debug, errors, info, warning, error information\n",
    "import logging\n",
    "logging.basicConfig(filename='example.log', encoding='utf-8', level=logging.DEBUG)\n",
    "\n",
    "# pickle to save dictionary in file\n",
    "import pickle \n",
    "\n",
    "# helper functions\n",
    "import sys\n",
    "# caution: path[0] is reserved for script path (or '' in REPL)\n",
    "cwd = os.path.normpath(os.getcwd() + os.sep + os.pardir + os.sep + os.pardir)\n",
    "sys.path.insert(1, cwd + \"/src/d00_utils\") \n",
    "from federated_helper_functions import *\n",
    "from model_helper_functions import *\n",
    "from windowgenerator import *\n",
    "\n",
    "\n",
    "#Data Analytics\n",
    "\n",
    "print(\"Get data\")\n",
    "# get current working directory and go back one folder to main working directory\n",
    "cwd = os.path.normpath(os.getcwd() + os.sep + os.pardir + os.sep + os.pardir)\n",
    "#Read CSV file to pandas dataframe; encoding= 'unicode_escape': Decode from Latin-1 source code. Default UTF-8.\n",
    "df = pd.read_csv(cwd+'/data/d03_data_processed/d03_data_processed.csv', encoding= 'unicode_escape', index_col='Date')\n",
    "#Display smart meter names and amount\n",
    "smart_meter_names = df.columns[2:-4]\n",
    "print(\"Selected clients: \", len(smart_meter_names))\n",
    "\n",
    "#Get clustered clients\n",
    "N_CLUSTERS = 6\n",
    "y = np.loadtxt(cwd+'/data/d04_clients_clustered/d04_clients_clustered.csv', delimiter=',').astype(int)\n",
    "print(\"Clustered clients: \", y)\n",
    "\n",
    "# Make Datasets for the 33 clients and for 5 and 7 features\n",
    "ds_dict = makeDatasetsForclientsAndfeatures(smart_meter_names, df)\n",
    "print(\"Created dictionary with datasets\")\n",
    "\n",
    "#Set Hyperparameter\n",
    "#Data Shape\n",
    "OUT_STEPS = [12, 24] #Next 12 or 24 hours\n",
    "NUM_FEATURES = [5, 7] # [F_T, F_TW] load_value, hour sin, hour cos, dayofweek sin, dayofweek cos + (temp, rhum)\n",
    "INPUT_STEPS = 24\n",
    "INPUT_SHAPE = [(INPUT_STEPS, NUM_FEATURES[0]), (INPUT_STEPS, NUM_FEATURES[1])]\n",
    "\n",
    "#LSTM\n",
    "NUM_LSTM_LAYERS = 4\n",
    "NUM_LSTM_CELLS = 32\n",
    "NUM_LSTM_DENSE_LAYERS=1\n",
    "NUM_LSTM_DENSE_UNITS = 32\n",
    "LSTM_DROPOUT = 0.2\n",
    "\n",
    "#CNN\n",
    "CONV_WIDTH = 3\n",
    "NUM_CNN_LAYERS = 4\n",
    "NUM_CNN_FILTERS = 24\n",
    "NUM_CNN_DENSE_LAYERS = 1\n",
    "NUM_CNN_DENSE_UNITS = 32\n",
    "CNN_DROPOUT = 0.2\n",
    "\n",
    "#Federated Learning\n",
    "comms_round = 20\n",
    "#Training epochs\n",
    "MAX_EPOCHS = 2\n",
    "\n",
    "# Create Windows \n",
    "windows_dict = createDataWindows(y, smart_meter_names, INPUT_STEPS, OUT_STEPS, ds_dict, N_CLUSTERS)\n",
    "print(\"Created Data windows\")\n",
    "\n",
    "windows_dict = {k: v for k, v in windows_dict.items() if k == 4}\n",
    "print(windows_dict)\n",
    "\n",
    "# Federated Learning\n",
    "# Set random seed for as reproducible results as possible\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "#Create Global models\n",
    "global_LSTM_models, global_CNN_models, global_Transformer_models = createGlobalModelsForClusters(\n",
    "        windows_dict, INPUT_SHAPE[0], OUT_STEPS[1], NUM_FEATURES[0], 'Federated_LSTM_F5_H24', 'Federated_CNN_F5_H24', 'Federated_Transformer_F5_H24',\n",
    "        NUM_LSTM_CELLS, NUM_LSTM_LAYERS, NUM_LSTM_DENSE_LAYERS, NUM_LSTM_DENSE_UNITS, LSTM_DROPOUT, \n",
    "        CONV_WIDTH, NUM_CNN_LAYERS, NUM_CNN_FILTERS, NUM_CNN_DENSE_LAYERS, NUM_CNN_DENSE_UNITS, CNN_DROPOUT, \n",
    "    )\n",
    "# Save global models to not loose any data when experiment crashes\n",
    "initiallySaveAllGlobalModels(cwd, global_LSTM_models, global_CNN_models, global_Transformer_models)\n",
    "print(\"Created and saved global models for each cluster\")\n",
    "\n",
    "# Iterate through federated learning for number of training rounds\n",
    "\n",
    "# For testing only 2 federated rounds\n",
    "# for idx_com, comm_round in enumerate(range(3,5)):\n",
    "for idx_com, comm_round in enumerate(range(comms_round)):\n",
    "\n",
    "    # Clear terminal and print current training round\n",
    "    IPython.display.clear_output()\n",
    "    print(\"Started Federated training round ----------\", idx_com+1, \"/\", comms_round)\n",
    "\n",
    "    #Train and update models for each cluster\n",
    "    for idx, cluster in enumerate(windows_dict):\n",
    "\n",
    "        print(\"Cluster--------\", idx+1, \"/\", N_CLUSTERS)\n",
    "\n",
    "        #Get global models\n",
    "        global_LSTM_model, global_CNN_model, global_Transformer_model = loadGlobalModels( \n",
    "            cwd, global_LSTM_models, global_CNN_models, global_Transformer_models, idx, idx_com\n",
    "        )\n",
    "        # Get the global model's weights \n",
    "        global_LSTM_weights, global_CNN_weights, global_Transformer_weights = getGlobalModelWeights(\n",
    "            global_LSTM_model, global_CNN_model, global_Transformer_model)\n",
    "        print(\"Got global models\")\n",
    "\n",
    "        #initial list for local model weights after scalling\n",
    "        scaled_local_weight_LSTM_list = list()\n",
    "        scaled_local_weight_CNN_list = list()\n",
    "        scaled_local_weight_Transformer_list = list()\n",
    "\n",
    "        client_names = getClientNamesOfCluster(windows_dict, cluster)\n",
    "\n",
    "        for client in windows_dict[cluster].keys():\n",
    "\n",
    "            #LSTM\n",
    "            local_LSTM_model = LSTM_Model().build(\n",
    "                INPUT_SHAPE[0], NUM_LSTM_CELLS, NUM_LSTM_LAYERS, NUM_LSTM_DENSE_LAYERS, NUM_LSTM_DENSE_UNITS,\n",
    "                LSTM_DROPOUT, OUT_STEPS[1], NUM_FEATURES[0], 'Federated_local_LSTM_F5_H24'\n",
    "            )\n",
    "            scaled_weights = compile_fit_set_weights(\n",
    "                local_LSTM_model, \n",
    "                global_LSTM_weights, \n",
    "                windows_dict[cluster][client][1], \n",
    "                client, \n",
    "                client_names, \n",
    "                MAX_EPOCHS, \n",
    "\n",
    "            )\n",
    "            scaled_local_weight_LSTM_list.append(scaled_weights)\n",
    "            print(\"Trained local LSTM\")\n",
    "\n",
    "            #CNN\n",
    "            local_CNN_model = CNN_Model().build(\n",
    "                INPUT_SHAPE[0], CONV_WIDTH, NUM_CNN_LAYERS, NUM_CNN_FILTERS, NUM_CNN_DENSE_LAYERS, NUM_CNN_DENSE_UNITS,\n",
    "                CNN_DROPOUT, OUT_STEPS[1], NUM_FEATURES[0],'Federated_local_CNN_F5_H24'\n",
    "            )    \n",
    "            scaled_weights = compile_fit_set_weights(\n",
    "                local_CNN_model, \n",
    "                global_CNN_weights, \n",
    "                windows_dict[cluster][client][1], #F5H24\n",
    "                client, \n",
    "                client_names, \n",
    "                MAX_EPOCHS, \n",
    "            )\n",
    "            scaled_local_weight_CNN_list.append(scaled_weights)\n",
    "            print(\"Trained local CNN\")\n",
    "\n",
    "            #Transformer\n",
    "            local_Transformer_model = Transformer_Model().build(\n",
    "                INPUT_SHAPE[0],OUT_STEPS[1],NUM_FEATURES[0],'Federated_local_Transformer_F5_H24'    \n",
    "            )\n",
    "            scaled_weights = compile_fit_set_weights(\n",
    "                local_Transformer_model, \n",
    "                global_Transformer_weights, \n",
    "                windows_dict[cluster][client][1], \n",
    "                client, \n",
    "                client_names, \n",
    "                MAX_EPOCHS, \n",
    "            )\n",
    "            scaled_local_weight_Transformer_list.append(scaled_weights)\n",
    "            print(\"Trained local Transformer\")\n",
    "            \n",
    "            #clear session to free memory after each communication round\n",
    "            K.clear_session()\n",
    "\n",
    "        #to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "        average_weights_LSTM = sum_scaled_weights(scaled_local_weight_LSTM_list)\n",
    "        average_weights_CNN = sum_scaled_weights(scaled_local_weight_CNN_list)\n",
    "        average_weights_Transformer = sum_scaled_weights(scaled_local_weight_Transformer_list)\n",
    "\n",
    "        #update global model \n",
    "        global_LSTM_models[idx].set_weights(average_weights_LSTM)\n",
    "        global_CNN_models[idx].set_weights(average_weights_CNN)\n",
    "        global_Transformer_models[idx].set_weights(average_weights_Transformer)\n",
    "\n",
    "        #Save global models\n",
    "        saveGlobalModels(cwd, global_LSTM_models[idx], global_CNN_models[idx], global_Transformer_models[idx], idx, idx_com)\n",
    "        print(\"Saved Global models\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
