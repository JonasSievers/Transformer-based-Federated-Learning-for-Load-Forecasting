{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de650f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports -----------------------------------------------------------------------------\n",
    "#Pandas: Reading and analyzing data\n",
    "import pandas as pd\n",
    "#Numerical calcuations\n",
    "import numpy as np\n",
    "#statistical data visualization\n",
    "import seaborn as sns\n",
    "#Use Dates in Datetime Format\n",
    "import datetime\n",
    "#Tensorflow\n",
    "#Evaluate models\n",
    "import math\n",
    "#Callback to logg model fitting time\n",
    "import time\n",
    "from timeit import default_timer as timer\n",
    "#Clear output after training\n",
    "import IPython\n",
    "import IPython.display\n",
    "#Normalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#Evaluate MSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#plot numpy array\n",
    "import matplotlib.pyplot as plt\n",
    "#Keras: Open-Source deep-learning library \n",
    "from tensorflow import keras\n",
    "#Building blocks of NN in Keras\n",
    "from tensorflow.keras import layers\n",
    "#EarlyStop to stop training early\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import backend as K\n",
    "#K-Means Clustering\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "import tensorflow as tf\n",
    "#Path for model Checkpoint\n",
    "import os\n",
    "#Helper Class (Export Notebook as .py)\n",
    "from windowgenerator import WindowGenerator\n",
    "from federated_helper_functions import *\n",
    "#Logging of debug, errors, info, warning, error\n",
    "import logging\n",
    "logging.basicConfig(filename='example.log', encoding='utf-8', level=logging.DEBUG)\n",
    "\n",
    "#Data Analytics ---------------------------------------------------------------------------\n",
    "\n",
    "#Define Path to datafile\n",
    "user = 'rs1044' #'Jonas'\n",
    "filename = '00Data_KIT_2019-01-01_to_2021_12_31.csv'\n",
    "path = 'C:/Users/' + user + '/bwSyncShare/02Python Code/Data/' + filename\n",
    "\n",
    "#1.1 Data collection & Visualization\n",
    "#Read CSV file to pandas dataframe; encoding= 'unicode_escape': Decode from Latin-1 source code. Default UTF-8.\n",
    "df = pd.read_csv(path, encoding= 'unicode_escape', index_col='Date')\n",
    "df.index = pd.to_datetime(df.index)\n",
    "df_feature = df.copy()\n",
    "#Drop columns with insignificant weather features\n",
    "df.drop(columns=['prcp', 'wdir','pres', 'coco','dwpt', 'wpgt','wspd'], axis=1, inplace=True)\n",
    "df['hour'] = df.index.hour\n",
    "df['dayofweek'] = df.index.dayofweek\n",
    "#Display smart meter names and amount\n",
    "smart_meter_names = df.columns[2:-2]\n",
    "print(len(smart_meter_names))\n",
    "#Convert Time String to seconds\n",
    "date_time = pd.to_datetime(df.index, format='%d.%m.%Y %H:%M:%S')\n",
    "timestamp_s = date_time.map(pd.Timestamp.timestamp)\n",
    "#Sine cosine Scaling\n",
    "hour = 60*60\n",
    "df['hour sin'] = np.sin(timestamp_s * (2 * np.pi / hour))\n",
    "df['hour cos'] = np.cos(timestamp_s * (2 * np.pi / hour))\n",
    "df.drop('hour', axis=1, inplace=True)\n",
    "daysinweek = 7*24*60*60\n",
    "df['dayofweek sin'] = np.sin(timestamp_s * (2 * np.pi / daysinweek))\n",
    "df['dayofweek cos'] = np.cos(timestamp_s * (2 * np.pi / daysinweek))\n",
    "df.drop('dayofweek', axis=1, inplace=True)\n",
    "#Min Max Sclaing\n",
    "col_names = df.columns\n",
    "features = df[col_names]\n",
    "scaler = MinMaxScaler().fit(features.values)\n",
    "features = scaler.transform(features.values)\n",
    "df = pd.DataFrame(features, columns = col_names, index=df.index)\n",
    "\n",
    "#K-menas Clustering\n",
    "N_CLUSTERS=6\n",
    "\"\"\"\n",
    "#New Dataframe for clustering data without time and weather features\n",
    "time_series_data = df.drop(['temp', 'rhum', 'hour sin', 'hour cos', 'dayofweek sin', 'dayofweek cos'], axis=1)\n",
    "time_series_data.reset_index(inplace=True)\n",
    "#Downsample Data to improve Clustering runtime\n",
    "undersample_data = time_series_data.loc[np.linspace(time_series_data.index.min(),time_series_data.index.max(),3000).astype(int)]\n",
    "undersample_data = undersample_data.reset_index().drop('index',axis=1)\n",
    "#Performe K-means clustering with Dynamic Time Warping\n",
    "data_array = np.array(undersample_data.T.drop('Date').values)\n",
    "model = TimeSeriesKMeans(n_clusters=N_CLUSTERS, metric=\"dtw\", max_iter=40)\n",
    "model.fit(data_array)\n",
    "y=model.predict(data_array)\n",
    "x = undersample_data.Date\n",
    "print(\"Clustering results: \", y)\n",
    "\"\"\"\n",
    "y=[2, 2, 4, 4, 1, 5, 1, 5, 1, 5, 5, 5, 5, 1, 3, 0, 3, 0, 5, 1, 3, 3, 3, 3, 5, 2, 2, 1, 3, 0, 0, 1, 2]\n",
    "print(\"Clustering results: \", y)\n",
    "\n",
    "# Create Datasets for the 33 clients and for 5 and 7 features\n",
    "#ds_dict[smart_meter_names][0-5] \n",
    "#    -> 0:train_df_F7, 1: val_df_F7, 2: test_df_F7, 3: train_df_F5, 4: val_df_F5, 5: test_df_F5\n",
    "ds_dict = {}\n",
    "n = len(df)\n",
    "for client in smart_meter_names:   \n",
    "    train_df_F7 = df[0:int(n*0.7)][[client, 'temp', 'rhum', 'hour sin', 'hour cos', 'dayofweek sin', 'dayofweek cos']]\n",
    "    val_df_F7 = df[int(n*0.7):int(n*0.9)][[client, 'temp', 'rhum', 'hour sin', 'hour cos', 'dayofweek sin', 'dayofweek cos']]\n",
    "    test_df_F7 = df[int(n*0.9):][[client, 'temp', 'rhum', 'hour sin', 'hour cos', 'dayofweek sin', 'dayofweek cos']]\n",
    "    \n",
    "    train_df_F5 = df[0:int(n*0.7)][[client, 'hour sin', 'hour cos', 'dayofweek sin', 'dayofweek cos']]\n",
    "    val_df_F5 = df[int(n*0.7):int(n*0.9)][[client, 'hour sin', 'hour cos', 'dayofweek sin', 'dayofweek cos']]\n",
    "    test_df_F5 = df[int(n*0.9):][[client, 'hour sin', 'hour cos', 'dayofweek sin', 'dayofweek cos']]\n",
    "    \n",
    "    ds_dict[client] = [train_df_F7, val_df_F7, test_df_F7, train_df_F5, val_df_F5, test_df_F5]\n",
    "\n",
    "#Initialize results\n",
    "final_dict = {}\n",
    "final_dict['Federated'] = {}\n",
    "final_dict['Federated']['LSTM'] = {}\n",
    "final_dict['Federated']['LSTM']['H12'] = {}\n",
    "final_dict['Federated']['LSTM']['H12']['F5'] = {}\n",
    "final_dict['Federated']['LSTM']['H12']['F7'] = {}\n",
    "#----------------------------------------------\n",
    "final_dict['Federated']['LSTM']['H24'] = {}\n",
    "final_dict['Federated']['LSTM']['H24']['F5'] = {}\n",
    "final_dict['Federated']['LSTM']['H24']['F7'] = {}\n",
    "\n",
    "final_dict['Federated']['CNN'] = {}\n",
    "final_dict['Federated']['CNN']['H12'] = {}\n",
    "final_dict['Federated']['CNN']['H12']['F5'] = {}\n",
    "final_dict['Federated']['CNN']['H12']['F7'] = {}\n",
    "#----------------------------------------------\n",
    "final_dict['Federated']['CNN']['H24'] = {}\n",
    "final_dict['Federated']['CNN']['H24']['F5'] = {}\n",
    "final_dict['Federated']['CNN']['H24']['F7'] = {}\n",
    "\n",
    "final_dict['Federated']['Transformer'] = {}\n",
    "final_dict['Federated']['Transformer']['H12'] = {}\n",
    "final_dict['Federated']['Transformer']['H12']['F5'] = {}\n",
    "final_dict['Federated']['Transformer']['H12']['F7'] = {}\n",
    "#----------------------------------------------\n",
    "final_dict['Federated']['Transformer']['H24'] = {}\n",
    "final_dict['Federated']['Transformer']['H24']['F5'] = {}\n",
    "final_dict['Federated']['Transformer']['H24']['F7'] = {}\n",
    "\n",
    "#Set Hyperparameter ------------------------------------------------------------------------------\n",
    "OUT_STEPS = [12, 24] #Next 12 or 24 hours\n",
    "NUM_FEATURES = [5, 7] # [F_T, F_TW] load_value, hour sin, hour cos, dayofweek sin, dayofweek cos + (temp, rhum)\n",
    "INPUT_STEPS = 24\n",
    "INPUT_SHAPE = [(INPUT_STEPS, NUM_FEATURES[0]), (INPUT_STEPS, NUM_FEATURES[1])]\n",
    "\n",
    "#All models\n",
    "MAX_EPOCHS = 2\n",
    "\n",
    "#LSTM\n",
    "NUM_LSTM_LAYERS = 4\n",
    "NUM_LSTM_CELLS = 32\n",
    "NUM_LSTM_DENSE_LAYERS=1\n",
    "NUM_LSTM_DENSE_UNITS = 32\n",
    "LSTM_DROPOUT = 0.2\n",
    "\n",
    "#CNN\n",
    "CONV_WIDTH = 3\n",
    "NUM_CNN_LAYERS = 4\n",
    "NUM_CNN_FILTERS = 24\n",
    "NUM_CNN_DENSE_LAYERS = 1\n",
    "NUM_CNN_DENSE_UNITS = 32\n",
    "CNN_DROPOUT = 0.2\n",
    "\n",
    "#Federated Learning\n",
    "comms_round = 20\n",
    "\n",
    "#Windowing-----------------------------------------------------------------\n",
    "#ds_dict[smart_meter_names][0-5] \n",
    "#    -> 0:train_df_F7, 1: val_df_F7, 2: test_df_F7, 3: train_df_F5, 4: val_df_F5, 5: test_df_F5\n",
    "\n",
    "#windows_dict[cluster 0-5][client_i_smart_meter_names][0-3] \n",
    "#    -> 0:window_F5_H12 , 1:window_F5_H24 , 2:window_F7_H12 , 3:window_F7_H24\n",
    "windows_dict = {k: {} for k in range(N_CLUSTERS)}\n",
    "\n",
    "for i, client in enumerate(smart_meter_names):\n",
    "    #window_F5_H12\n",
    "    window_F5_H12 = WindowGenerator(\n",
    "        input_width=INPUT_STEPS, label_width=OUT_STEPS[0], shift=OUT_STEPS[0], \n",
    "        train_df = ds_dict[client][3], val_df = ds_dict[client][4], test_df = ds_dict[client][5], label_columns=[client]\n",
    "    )\n",
    "    example_window = tf.stack([np.array(ds_dict[client][3][10100:10100+window_F5_H12.total_window_size]),\n",
    "                               np.array(ds_dict[client][3][2000:2000+window_F5_H12.total_window_size]),\n",
    "                               np.array(ds_dict[client][3][3000:3000+window_F5_H12.total_window_size])])\n",
    "    example_inputs, example_labels = window_F5_H12.split_window(example_window)\n",
    "    window_F5_H12.example = example_inputs, example_labels\n",
    "\n",
    "    #window_F5_H24\n",
    "    window_F5_H24 = WindowGenerator(\n",
    "        input_width=INPUT_STEPS, label_width=OUT_STEPS[1], shift=OUT_STEPS[1], \n",
    "        train_df = ds_dict[client][3], val_df = ds_dict[client][4], test_df = ds_dict[client][5], label_columns=[client]\n",
    "    )\n",
    "    example_window = tf.stack([np.array(ds_dict[client][3][10100:10100+window_F5_H24.total_window_size]),\n",
    "                               np.array(ds_dict[client][3][2000:2000+window_F5_H24.total_window_size]),\n",
    "                               np.array(ds_dict[client][3][3000:3000+window_F5_H24.total_window_size])])\n",
    "    example_inputs, example_labels = window_F5_H24.split_window(example_window)\n",
    "    window_F5_H24.example = example_inputs, example_labels\n",
    "\n",
    "    #window_F7_H12\n",
    "    window_F7_H12 = WindowGenerator(\n",
    "        input_width=INPUT_STEPS, label_width=OUT_STEPS[0], shift=OUT_STEPS[0], \n",
    "        train_df = ds_dict[client][0], val_df = ds_dict[client][1], test_df = ds_dict[client][2], label_columns=[client]\n",
    "    )\n",
    "    example_window = tf.stack([np.array(ds_dict[client][0][10100:10100+window_F7_H12.total_window_size]),\n",
    "                               np.array(ds_dict[client][0][2000:2000+window_F7_H12.total_window_size]),\n",
    "                               np.array(ds_dict[client][0][3000:3000+window_F7_H12.total_window_size])])\n",
    "    example_inputs, example_labels = window_F7_H12.split_window(example_window)\n",
    "    window_F7_H12.example = example_inputs, example_labels\n",
    "\n",
    "    #window_F5_H24\n",
    "    window_F7_H24 = WindowGenerator(\n",
    "        input_width=INPUT_STEPS, label_width=OUT_STEPS[1], shift=OUT_STEPS[1], \n",
    "        train_df = ds_dict[client][0], val_df = ds_dict[client][1], test_df = ds_dict[client][2], label_columns=[client]\n",
    "    )\n",
    "    example_window = tf.stack([np.array(ds_dict[client][0][10100:10100+window_F7_H24.total_window_size]),\n",
    "                               np.array(ds_dict[client][0][2000:2000+window_F7_H24.total_window_size]),\n",
    "                               np.array(ds_dict[client][0][3000:3000+window_F7_H24.total_window_size])])\n",
    "    example_inputs, example_labels = window_F7_H24.split_window(example_window)\n",
    "    window_F7_H24.example = example_inputs, example_labels\n",
    "    \n",
    "    windows_dict[y[i]]['{}_{}_{}'.format('client', i+1, client)] = [window_F5_H12, window_F5_H24, window_F7_H12, window_F7_H24]\n",
    "    \n",
    "    \n",
    "def compile_fit_set_weights(local_model, global_weights, window, client, client_names, model_type):\n",
    "    \n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=2,mode='min')\n",
    "    local_model.compile(\n",
    "        loss=tf.keras.losses.MeanSquaredError(),\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        metrics=[\n",
    "            tf.keras.metrics.RootMeanSquaredError(), \n",
    "            tf.keras.metrics.MeanAbsolutePercentageError(),\n",
    "            tf.keras.metrics.MeanAbsoluteError(),\n",
    "        ]\n",
    "    )\n",
    "    #set local model weight to the weight of the global model\n",
    "    local_model.set_weights(global_weights)\n",
    "    #fit local model with client's data\n",
    "    local_model.fit(window.train, epochs=MAX_EPOCHS, verbose=1, validation_data=window.val,\n",
    "                      callbacks=[early_stopping, create_model_checkpoint(save_path=f\"model_experiments/Federated/{local_model.name}/{client}\"), timetaken]\n",
    "                   )\n",
    "    \n",
    "    #scale the model weights and add to list        \n",
    "    scaling_factor = weight_scalling_factor(window.train, client, client_names)\n",
    "    scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
    "    \n",
    "    if (model_type == 'LSTM'):\n",
    "        scaled_local_weight_LSTM_list.append(scaled_weights)\n",
    "    elif (model_type == 'CNN'):\n",
    "        scaled_local_weight_CNN_list.append(scaled_weights)\n",
    "    elif (model_type == 'Transformer'):\n",
    "        scaled_local_weight_Transformer_list.append(scaled_weights)\n",
    "    \n",
    "    #clear session to free memory after each communication round\n",
    "    K.clear_session()\n",
    "\n",
    "# Set random seed for as reproducible results as possible\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "#Federated Training ------------------------------------------------------\n",
    "#Track memory usage\n",
    "# importing the module\n",
    "import tracemalloc\n",
    "\n",
    "# starting the monitoring\n",
    "tracemalloc.start()\n",
    "\n",
    "try:\n",
    "    ### Features 7, Horizon 24\n",
    "    global_LSTM_model = []\n",
    "    global_CNN_model = []\n",
    "    global_Transformer_model = []\n",
    "\n",
    "    for idx, cluster in enumerate(windows_dict):\n",
    "\n",
    "        #Build Models\n",
    "        global_LSTM_model.append(LSTM_Model().build(\n",
    "            input_shape = INPUT_SHAPE[1], \n",
    "            num_LSTM_cells = NUM_LSTM_CELLS,\n",
    "            num_LSTM_layers = NUM_LSTM_LAYERS,\n",
    "            num_LSTM_dense_layers = NUM_LSTM_DENSE_LAYERS,\n",
    "            num_LSTM_dense_units = NUM_LSTM_DENSE_UNITS,\n",
    "            LSTM_dropout = LSTM_DROPOUT,\n",
    "            output_steps = OUT_STEPS[1],\n",
    "            num_features = NUM_FEATURES[1],\n",
    "            model_name = 'Federated_LSTM_F7_H24'\n",
    "        ))\n",
    "        #CNN        \n",
    "        global_CNN_model.append(CNN_Model().build(\n",
    "            input_shape = INPUT_SHAPE[1], \n",
    "            conv_width = CONV_WIDTH,\n",
    "            num_CNN_layers = NUM_CNN_LAYERS,\n",
    "            num_CNN_filters = NUM_CNN_FILTERS,\n",
    "            num_CNN_dense_layers = NUM_CNN_DENSE_LAYERS,\n",
    "            num_CNN_dense_units = NUM_CNN_DENSE_UNITS,\n",
    "            CNN_dropout = CNN_DROPOUT,\n",
    "            output_steps = OUT_STEPS[1],\n",
    "            num_features = NUM_FEATURES[1],\n",
    "            model_name = 'Federated_CNN_F7_H24'\n",
    "        ))\n",
    "        #Transformer\n",
    "        global_Transformer_model.append(Transformer_Model().build(\n",
    "            input_shape = INPUT_SHAPE[1],\n",
    "            output_steps = OUT_STEPS[1],\n",
    "            num_features = NUM_FEATURES[1],\n",
    "            model_name = 'Federated_Transformer_F7_H24'    \n",
    "        ))\n",
    "\n",
    "    #windows_dict[client_i_smart_meter_names][0-3] \n",
    "    #    -> 0:window_F5_H12 , 1:window_F5_H24 , 2:window_F7_H12 , 3:window_F7_H24\n",
    "\n",
    "    #commence global training loop\n",
    "    for idx_com, comm_round in enumerate(range(comms_round)):\n",
    "\n",
    "        for idx, cluster in enumerate(windows_dict):\n",
    "            IPython.display.clear_output()\n",
    "            print(\"--------Federated Round---\", idx_com+1, \"/\", comms_round, \"---Cluster--\", idx+1, \"/5\")\n",
    "            # Get the global model's weights \n",
    "            global_LSTM_weights = global_LSTM_model[idx].get_weights()\n",
    "            global_CNN_weights = global_CNN_model[idx].get_weights()\n",
    "            global_Transformer_weights = global_Transformer_model[idx].get_weights()\n",
    "\n",
    "            #initial list for local model weights after scalling\n",
    "            scaled_local_weight_LSTM_list = list()\n",
    "            scaled_local_weight_CNN_list = list()\n",
    "            scaled_local_weight_Transformer_list = list()\n",
    "\n",
    "\n",
    "            #Get names of clients within cluster\n",
    "            client_names = list()\n",
    "            for client in windows_dict[cluster]:\n",
    "                client_names.append(client)\n",
    "\n",
    "            for client in windows_dict[cluster].keys():\n",
    "\n",
    "                #LSTM\n",
    "                local_LSTM_model = LSTM_Model().build(\n",
    "                    INPUT_SHAPE[1], NUM_LSTM_CELLS, NUM_LSTM_LAYERS, NUM_LSTM_DENSE_LAYERS, NUM_LSTM_DENSE_UNITS,\n",
    "                    LSTM_DROPOUT, OUT_STEPS[1], NUM_FEATURES[1], 'Federated_local_LSTM_F7_H24'\n",
    "                )\n",
    "                compile_fit_set_weights(local_LSTM_model, global_LSTM_weights, windows_dict[cluster][client][3], client, client_names, 'LSTM')\n",
    "\n",
    "                #CNN\n",
    "                local_CNN_model = CNN_Model().build(\n",
    "                    INPUT_SHAPE[1], CONV_WIDTH, NUM_CNN_LAYERS, NUM_CNN_FILTERS, NUM_CNN_DENSE_LAYERS, NUM_CNN_DENSE_UNITS,\n",
    "                    CNN_DROPOUT, OUT_STEPS[1], NUM_FEATURES[1],'Federated_local_CNN_F7_H24'\n",
    "                )    \n",
    "                compile_fit_set_weights(local_CNN_model, global_CNN_weights, windows_dict[cluster][client][3], client, client_names, 'CNN')\n",
    "\n",
    "                #Transformer\n",
    "                local_Transformer_model = Transformer_Model().build(\n",
    "                    INPUT_SHAPE[1],OUT_STEPS[1],NUM_FEATURES[1],'Federated_local_Transformer_F7_H24'    \n",
    "                )\n",
    "                compile_fit_set_weights(local_Transformer_model, global_Transformer_weights, windows_dict[cluster][client][3], client, client_names, 'Transformer')\n",
    "\n",
    "            #to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "            average_weights_LSTM = sum_scaled_weights(scaled_local_weight_LSTM_list)\n",
    "            average_weights_CNN = sum_scaled_weights(scaled_local_weight_CNN_list)\n",
    "            average_weights_Transformer = sum_scaled_weights(scaled_local_weight_Transformer_list)\n",
    "            #update global model \n",
    "            global_LSTM_model[idx].set_weights(average_weights_LSTM)\n",
    "            global_CNN_model[idx].set_weights(average_weights_CNN)\n",
    "            global_Transformer_model[idx].set_weights(average_weights_Transformer)\n",
    "\n",
    "    #Evaluate Results\n",
    "    forecasts_dict_LSTM_F7_H24 = {k: {} for k in range(N_CLUSTERS)}\n",
    "    forecasts_dict_CNN_F7_H24 = {k: {} for k in range(N_CLUSTERS)}\n",
    "    forecasts_dict_Transformer_F7_H24 = {k: {} for k in range(N_CLUSTERS)}\n",
    "\n",
    "    for idx, cluster in enumerate(windows_dict):\n",
    "        #Get names of clients within cluster\n",
    "        client_names = list()\n",
    "        for client in windows_dict[cluster]:\n",
    "            client_names.append(client)\n",
    "\n",
    "        for i, client in enumerate(windows_dict[cluster].keys()):\n",
    "            IPython.display.clear_output()\n",
    "            print(\"-------------Cluster----\", cluster, \"-----\", client,\"--------\", i+1, \"/\", len(client_names))\n",
    "\n",
    "            #LSTM\n",
    "            model_evaluation_test = test_model(windows_dict[cluster][client][3], global_LSTM_model[idx], client, MAX_EPOCHS)\n",
    "            #Save\n",
    "            forecasts_dict_LSTM_F7_H24[cluster][client] = {\n",
    "                'MSE':model_evaluation_test[0], 'RMSE':model_evaluation_test[1], 'MAPE':model_evaluation_test[2],\n",
    "                'MAE':model_evaluation_test[3], 'Time':((timetaken.logs[-1][1]) / (timetaken.logs[-1][0]+1)) \n",
    "            }\n",
    "            #CNN\n",
    "            model_evaluation_test = test_model(windows_dict[cluster][client][3], global_CNN_model[idx], client, MAX_EPOCHS)\n",
    "            #Save\n",
    "            forecasts_dict_CNN_F7_H24[cluster][client] = {\n",
    "                'MSE':model_evaluation_test[0], 'RMSE':model_evaluation_test[1], 'MAPE':model_evaluation_test[2],\n",
    "                'MAE':model_evaluation_test[3], 'Time':((timetaken.logs[-1][1]) / (timetaken.logs[-1][0]+1)) \n",
    "            }    \n",
    "            #Transformer\n",
    "            model_evaluation_test = test_model(windows_dict[cluster][client][3], global_Transformer_model[idx], client, MAX_EPOCHS)\n",
    "            #Save\n",
    "            forecasts_dict_Transformer_F7_H24[cluster][client] = {\n",
    "                'MSE':model_evaluation_test[0], 'RMSE':model_evaluation_test[1], 'MAPE':model_evaluation_test[2],\n",
    "                'MAE':model_evaluation_test[3], 'Time':((timetaken.logs[-1][1]) / (timetaken.logs[-1][0]+1)) \n",
    "            }\n",
    "\n",
    "    final_dict['Federated']['LSTM']['H24']['F7'] = forecasts_dict_LSTM_F7_H24\n",
    "    final_dict['Federated']['CNN']['H24']['F7'] = forecasts_dict_CNN_F7_H24\n",
    "    final_dict['Federated']['Transformer']['H24']['F7'] = forecasts_dict_Transformer_F7_H24\n",
    "\n",
    "    import pickle \n",
    "    with open('Dictionaries_Results/Federated_resultsF5H12.pkl', 'wb') as f:\n",
    "        pickle.dump(final_dict, f)\n",
    " \n",
    "except Exception, e: \n",
    "    logger.error('Failed to upload to ftp: '+ str(e))\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
