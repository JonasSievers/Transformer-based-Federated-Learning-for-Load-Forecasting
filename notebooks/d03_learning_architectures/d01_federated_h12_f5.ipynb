{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61b3fd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "\n",
    "# pandas for reading and analyzing data\n",
    "import pandas as pd\n",
    "# numpy for numerical calcuations\n",
    "import numpy as np\n",
    "# seaborn for statistical data visualization\n",
    "import seaborn as sns\n",
    "# datetime to use dates in datetime format\n",
    "import datetime\n",
    "# math to calculate model evaluation steps\n",
    "import math\n",
    "# sklearn for minMaxSclaing and mse\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# matplotlib to plot numpy array\n",
    "import matplotlib.pyplot as plt\n",
    "#tslearn for K-Means Clustering\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "# os to find path of files \n",
    "import os\n",
    "\n",
    "# tensorflow as machine learning library\n",
    "import tensorflow as tf\n",
    "# keras as open-source deep-learning library \n",
    "from tensorflow import keras\n",
    "# building blocks of NN in Keras\n",
    "from tensorflow.keras import layers\n",
    "# earlyStop to stop training early\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# IPython to Clear terminal output\n",
    "import IPython\n",
    "import IPython.display\n",
    "# time and timeit to provie a callback to logg model fitting time\n",
    "import time\n",
    "from timeit import default_timer as timer\n",
    "# logging to logg debug, errors, info, warning, error information\n",
    "import logging\n",
    "logging.basicConfig(filename='example.log', encoding='utf-8', level=logging.DEBUG)\n",
    "# tracemalloc to trace memory usage\n",
    "#import tracemalloc\n",
    "#tracemalloc.start()\n",
    "# pickle to save dictionary in file\n",
    "import pickle \n",
    "\n",
    "# helper functions and classes (exported notebooks as .py)\n",
    "#from ../01ScriptsHelperFunctions/windowgenerator import WindowGenerator\n",
    "#from ../01ScriptsHelperFunctions/federated_helper_functions import *\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1259b801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "#Data Analytics\n",
    "\n",
    "# get current working directory and go back one folder to main working directory\n",
    "cwd = os.path.normpath(os.getcwd() + os.sep + os.pardir)\n",
    "# set path to load data file\n",
    "load_data_path = '/transformerBasedFLForSecureSTLFInSG/data/d03_data_processed.csv'\n",
    "path = cwd + load_data_path\n",
    "  \n",
    "#Read CSV file to pandas dataframe; encoding= 'unicode_escape': Decode from Latin-1 source code. Default UTF-8.\n",
    "df = pd.read_csv(path, encoding= 'unicode_escape', index_col='Date')\n",
    "#Display smart meter names and amount\n",
    "smart_meter_names = df.columns[2:-4]\n",
    "print(len(smart_meter_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff6141c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "multiple exception types must be parenthesized (1437979895.py, line 430)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn [1], line 430\u001b[1;36m\u001b[0m\n\u001b[1;33m    except Exception, e:\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m multiple exception types must be parenthesized\n"
     ]
    }
   ],
   "source": [
    "y=[2, 2, 4, 4, 1, 5, 1, 5, 1, 5, 5, 5, 5, 1, 3, 0, 3, 0, 5, 1, 3, 3, 3, 3, 5, 2, 2, 1, 3, 0, 0, 1, 2]\n",
    "print(\"Clustering results: \", y)\n",
    "\n",
    "# Create Datasets for the 33 clients and for 5 and 7 features\n",
    "#ds_dict[smart_meter_names][0-5] \n",
    "#    -> 0:train_df_F7, 1: val_df_F7, 2: test_df_F7, 3: train_df_F5, 4: val_df_F5, 5: test_df_F5\n",
    "ds_dict = {}\n",
    "n = len(df)\n",
    "for client in smart_meter_names:   \n",
    "    train_df_F7 = df[0:int(n*0.7)][[client, 'temp', 'rhum', 'hour sin', 'hour cos', 'dayofweek sin', 'dayofweek cos']]\n",
    "    val_df_F7 = df[int(n*0.7):int(n*0.9)][[client, 'temp', 'rhum', 'hour sin', 'hour cos', 'dayofweek sin', 'dayofweek cos']]\n",
    "    test_df_F7 = df[int(n*0.9):][[client, 'temp', 'rhum', 'hour sin', 'hour cos', 'dayofweek sin', 'dayofweek cos']]\n",
    "    \n",
    "    train_df_F5 = df[0:int(n*0.7)][[client, 'hour sin', 'hour cos', 'dayofweek sin', 'dayofweek cos']]\n",
    "    val_df_F5 = df[int(n*0.7):int(n*0.9)][[client, 'hour sin', 'hour cos', 'dayofweek sin', 'dayofweek cos']]\n",
    "    test_df_F5 = df[int(n*0.9):][[client, 'hour sin', 'hour cos', 'dayofweek sin', 'dayofweek cos']]\n",
    "    \n",
    "    ds_dict[client] = [train_df_F7, val_df_F7, test_df_F7, train_df_F5, val_df_F5, test_df_F5]\n",
    "\n",
    "#Initialize results\n",
    "final_dict = {}\n",
    "final_dict['Federated'] = {}\n",
    "final_dict['Federated']['LSTM'] = {}\n",
    "final_dict['Federated']['LSTM']['H12'] = {}\n",
    "final_dict['Federated']['LSTM']['H12']['F5'] = {}\n",
    "final_dict['Federated']['LSTM']['H12']['F7'] = {}\n",
    "#----------------------------------------------\n",
    "final_dict['Federated']['LSTM']['H24'] = {}\n",
    "final_dict['Federated']['LSTM']['H24']['F5'] = {}\n",
    "final_dict['Federated']['LSTM']['H24']['F7'] = {}\n",
    "\n",
    "final_dict['Federated']['CNN'] = {}\n",
    "final_dict['Federated']['CNN']['H12'] = {}\n",
    "final_dict['Federated']['CNN']['H12']['F5'] = {}\n",
    "final_dict['Federated']['CNN']['H12']['F7'] = {}\n",
    "#----------------------------------------------\n",
    "final_dict['Federated']['CNN']['H24'] = {}\n",
    "final_dict['Federated']['CNN']['H24']['F5'] = {}\n",
    "final_dict['Federated']['CNN']['H24']['F7'] = {}\n",
    "\n",
    "final_dict['Federated']['Transformer'] = {}\n",
    "final_dict['Federated']['Transformer']['H12'] = {}\n",
    "final_dict['Federated']['Transformer']['H12']['F5'] = {}\n",
    "final_dict['Federated']['Transformer']['H12']['F7'] = {}\n",
    "#----------------------------------------------\n",
    "final_dict['Federated']['Transformer']['H24'] = {}\n",
    "final_dict['Federated']['Transformer']['H24']['F5'] = {}\n",
    "final_dict['Federated']['Transformer']['H24']['F7'] = {}\n",
    "\n",
    "#Set Hyperparameter ------------------------------------------------------------------------------\n",
    "OUT_STEPS = [12, 24] #Next 12 or 24 hours\n",
    "NUM_FEATURES = [5, 7] # [F_T, F_TW] load_value, hour sin, hour cos, dayofweek sin, dayofweek cos + (temp, rhum)\n",
    "INPUT_STEPS = 24\n",
    "INPUT_SHAPE = [(INPUT_STEPS, NUM_FEATURES[0]), (INPUT_STEPS, NUM_FEATURES[1])]\n",
    "\n",
    "#All models\n",
    "MAX_EPOCHS = 2\n",
    "\n",
    "#LSTM\n",
    "NUM_LSTM_LAYERS = 4\n",
    "NUM_LSTM_CELLS = 32\n",
    "NUM_LSTM_DENSE_LAYERS=1\n",
    "NUM_LSTM_DENSE_UNITS = 32\n",
    "LSTM_DROPOUT = 0.2\n",
    "\n",
    "#CNN\n",
    "CONV_WIDTH = 3\n",
    "NUM_CNN_LAYERS = 4\n",
    "NUM_CNN_FILTERS = 24\n",
    "NUM_CNN_DENSE_LAYERS = 1\n",
    "NUM_CNN_DENSE_UNITS = 32\n",
    "CNN_DROPOUT = 0.2\n",
    "\n",
    "#Federated Learning\n",
    "comms_round = 20\n",
    "\n",
    "#Windowing-----------------------------------------------------------------\n",
    "#ds_dict[smart_meter_names][0-5] \n",
    "#    -> 0:train_df_F7, 1: val_df_F7, 2: test_df_F7, 3: train_df_F5, 4: val_df_F5, 5: test_df_F5\n",
    "\n",
    "#windows_dict[cluster 0-5][client_i_smart_meter_names][0-3] \n",
    "#    -> 0:window_F5_H12 , 1:window_F5_H24 , 2:window_F7_H12 , 3:window_F7_H24\n",
    "windows_dict = {k: {} for k in range(N_CLUSTERS)}\n",
    "\n",
    "for i, client in enumerate(smart_meter_names):\n",
    "    #window_F5_H12\n",
    "    window_F5_H12 = WindowGenerator(\n",
    "        input_width=INPUT_STEPS, label_width=OUT_STEPS[0], shift=OUT_STEPS[0], \n",
    "        train_df = ds_dict[client][3], val_df = ds_dict[client][4], test_df = ds_dict[client][5], label_columns=[client]\n",
    "    )\n",
    "    example_window = tf.stack([np.array(ds_dict[client][3][10100:10100+window_F5_H12.total_window_size]),\n",
    "                               np.array(ds_dict[client][3][2000:2000+window_F5_H12.total_window_size]),\n",
    "                               np.array(ds_dict[client][3][3000:3000+window_F5_H12.total_window_size])])\n",
    "    example_inputs, example_labels = window_F5_H12.split_window(example_window)\n",
    "    window_F5_H12.example = example_inputs, example_labels\n",
    "\n",
    "    #window_F5_H24\n",
    "    window_F5_H24 = WindowGenerator(\n",
    "        input_width=INPUT_STEPS, label_width=OUT_STEPS[1], shift=OUT_STEPS[1], \n",
    "        train_df = ds_dict[client][3], val_df = ds_dict[client][4], test_df = ds_dict[client][5], label_columns=[client]\n",
    "    )\n",
    "    example_window = tf.stack([np.array(ds_dict[client][3][10100:10100+window_F5_H24.total_window_size]),\n",
    "                               np.array(ds_dict[client][3][2000:2000+window_F5_H24.total_window_size]),\n",
    "                               np.array(ds_dict[client][3][3000:3000+window_F5_H24.total_window_size])])\n",
    "    example_inputs, example_labels = window_F5_H24.split_window(example_window)\n",
    "    window_F5_H24.example = example_inputs, example_labels\n",
    "\n",
    "    #window_F7_H12\n",
    "    window_F7_H12 = WindowGenerator(\n",
    "        input_width=INPUT_STEPS, label_width=OUT_STEPS[0], shift=OUT_STEPS[0], \n",
    "        train_df = ds_dict[client][0], val_df = ds_dict[client][1], test_df = ds_dict[client][2], label_columns=[client]\n",
    "    )\n",
    "    example_window = tf.stack([np.array(ds_dict[client][0][10100:10100+window_F7_H12.total_window_size]),\n",
    "                               np.array(ds_dict[client][0][2000:2000+window_F7_H12.total_window_size]),\n",
    "                               np.array(ds_dict[client][0][3000:3000+window_F7_H12.total_window_size])])\n",
    "    example_inputs, example_labels = window_F7_H12.split_window(example_window)\n",
    "    window_F7_H12.example = example_inputs, example_labels\n",
    "\n",
    "    #window_F5_H24\n",
    "    window_F7_H24 = WindowGenerator(\n",
    "        input_width=INPUT_STEPS, label_width=OUT_STEPS[1], shift=OUT_STEPS[1], \n",
    "        train_df = ds_dict[client][0], val_df = ds_dict[client][1], test_df = ds_dict[client][2], label_columns=[client]\n",
    "    )\n",
    "    example_window = tf.stack([np.array(ds_dict[client][0][10100:10100+window_F7_H24.total_window_size]),\n",
    "                               np.array(ds_dict[client][0][2000:2000+window_F7_H24.total_window_size]),\n",
    "                               np.array(ds_dict[client][0][3000:3000+window_F7_H24.total_window_size])])\n",
    "    example_inputs, example_labels = window_F7_H24.split_window(example_window)\n",
    "    window_F7_H24.example = example_inputs, example_labels\n",
    "    \n",
    "    windows_dict[y[i]]['{}_{}_{}'.format('client', i+1, client)] = [window_F5_H12, window_F5_H24, window_F7_H12, window_F7_H24]\n",
    "    \n",
    "    \n",
    "def compile_fit_set_weights(local_model, global_weights, window, client, client_names, model_type):\n",
    "    \n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=2,mode='min')\n",
    "    local_model.compile(\n",
    "        loss=tf.keras.losses.MeanSquaredError(),\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        metrics=[\n",
    "            tf.keras.metrics.RootMeanSquaredError(), \n",
    "            tf.keras.metrics.MeanAbsolutePercentageError(),\n",
    "            tf.keras.metrics.MeanAbsoluteError(),\n",
    "        ]\n",
    "    )\n",
    "    #set local model weight to the weight of the global model\n",
    "    local_model.set_weights(global_weights)\n",
    "    #fit local model with client's data\n",
    "    local_model.fit(window.train, epochs=MAX_EPOCHS, verbose=1, validation_data=window.val,\n",
    "                      callbacks=[early_stopping, create_model_checkpoint(save_path=f\"model_experiments/Federated/{local_model.name}/{client}\"), timetaken]\n",
    "                   )\n",
    "    \n",
    "    #scale the model weights and add to list        \n",
    "    scaling_factor = weight_scalling_factor(window.train, client, client_names)\n",
    "    scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
    "    \n",
    "    if (model_type == 'LSTM'):\n",
    "        scaled_local_weight_LSTM_list.append(scaled_weights)\n",
    "    elif (model_type == 'CNN'):\n",
    "        scaled_local_weight_CNN_list.append(scaled_weights)\n",
    "    elif (model_type == 'Transformer'):\n",
    "        scaled_local_weight_Transformer_list.append(scaled_weights)\n",
    "    \n",
    "    #clear session to free memory after each communication round\n",
    "    K.clear_session()\n",
    "\n",
    "# Set random seed for as reproducible results as possible\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "\n",
    "#Federated Training ------------------------------------------------------\n",
    "#Track memory usage\n",
    "\n",
    "\n",
    "try:\n",
    "    ### Features 5, Horizon 12\n",
    "    global_LSTM_model = []\n",
    "    global_CNN_model = []\n",
    "    global_Transformer_model = []\n",
    "\n",
    "    for idx, cluster in enumerate(windows_dict):\n",
    "\n",
    "        #Build Models\n",
    "        global_LSTM_model.append(LSTM_Model().build(\n",
    "            input_shape = INPUT_SHAPE[0], \n",
    "            num_LSTM_cells = NUM_LSTM_CELLS,\n",
    "            num_LSTM_layers = NUM_LSTM_LAYERS,\n",
    "            num_LSTM_dense_layers = NUM_LSTM_DENSE_LAYERS,\n",
    "            num_LSTM_dense_units = NUM_LSTM_DENSE_UNITS,\n",
    "            LSTM_dropout = LSTM_DROPOUT,\n",
    "            output_steps = OUT_STEPS[0],\n",
    "            num_features = NUM_FEATURES[0],\n",
    "            model_name = 'Federated_LSTM_F5_H12'\n",
    "        ))\n",
    "        #CNN        \n",
    "        global_CNN_model.append(CNN_Model().build(\n",
    "            input_shape = INPUT_SHAPE[0], \n",
    "            conv_width = CONV_WIDTH,\n",
    "            num_CNN_layers = NUM_CNN_LAYERS,\n",
    "            num_CNN_filters = NUM_CNN_FILTERS,\n",
    "            num_CNN_dense_layers = NUM_CNN_DENSE_LAYERS,\n",
    "            num_CNN_dense_units = NUM_CNN_DENSE_UNITS,\n",
    "            CNN_dropout = CNN_DROPOUT,\n",
    "            output_steps = OUT_STEPS[0],\n",
    "            num_features = NUM_FEATURES[0],\n",
    "            model_name = 'Federated_CNN_F5_H12'\n",
    "        ))\n",
    "        #Transformer\n",
    "        global_Transformer_model.append(Transformer_Model().build(\n",
    "            input_shape = INPUT_SHAPE[0],\n",
    "            output_steps = OUT_STEPS[0],\n",
    "            num_features = NUM_FEATURES[0],\n",
    "            model_name = 'Federated_Transformer_F5_H12'    \n",
    "        ))\n",
    "    #windows_dict[client_i_smart_meter_names][0-3] \n",
    "    #    -> 0:window_F5_H12 , 1:window_F5_H24 , 2:window_F7_H12 , 3:window_F7_H24\n",
    "\n",
    "    #commence global training loop\n",
    "    for idx_com, comm_round in enumerate(range(comms_round)):\n",
    "\n",
    "        for idx, cluster in enumerate(windows_dict):\n",
    "            IPython.display.clear_output()\n",
    "            print(\"--------Federated Round---\", idx_com+1, \"/\", comms_round, \"---Cluster--\", idx+1, \"/5\")\n",
    "            # displaying the memory\n",
    "            print(\"Memory used: \", tracemalloc.get_traced_memory())\n",
    "            # Get the global model's weights \n",
    "            global_LSTM_weights = global_LSTM_model[idx].get_weights()\n",
    "            global_CNN_weights = global_CNN_model[idx].get_weights()\n",
    "            global_Transformer_weights = global_Transformer_model[idx].get_weights()\n",
    "\n",
    "            #initial list for local model weights after scalling\n",
    "            scaled_local_weight_LSTM_list = list()\n",
    "            scaled_local_weight_CNN_list = list()\n",
    "            scaled_local_weight_Transformer_list = list()\n",
    "\n",
    "\n",
    "            #Get names of clients within cluster\n",
    "            client_names = list()\n",
    "            for client in windows_dict[cluster]:\n",
    "                client_names.append(client)\n",
    "\n",
    "            for client in windows_dict[cluster].keys():\n",
    "                #LSTM\n",
    "                local_LSTM_model = LSTM_Model().build(\n",
    "                    INPUT_SHAPE[0], NUM_LSTM_CELLS, NUM_LSTM_LAYERS, NUM_LSTM_DENSE_LAYERS, NUM_LSTM_DENSE_UNITS,\n",
    "                    LSTM_DROPOUT, OUT_STEPS[0], NUM_FEATURES[0], 'Federated_local_LSTM_F5_H12'\n",
    "                )\n",
    "                compile_fit_set_weights(local_LSTM_model, global_LSTM_weights, windows_dict[cluster][client][0], client, client_names, 'LSTM')\n",
    "\n",
    "                #CNN\n",
    "                local_CNN_model = CNN_Model().build(\n",
    "                    INPUT_SHAPE[0], CONV_WIDTH, NUM_CNN_LAYERS, NUM_CNN_FILTERS, NUM_CNN_DENSE_LAYERS, NUM_CNN_DENSE_UNITS,\n",
    "                    CNN_DROPOUT, OUT_STEPS[0], NUM_FEATURES[0],'Federated_local_CNN_F5_H24'\n",
    "                )    \n",
    "                compile_fit_set_weights(local_CNN_model, global_CNN_weights, windows_dict[cluster][client][0], client, client_names, 'CNN')\n",
    "\n",
    "                #Transformer\n",
    "                local_Transformer_model = Transformer_Model().build(\n",
    "                    INPUT_SHAPE[0],OUT_STEPS[0],NUM_FEATURES[0],'Federated_local_Transformer_F5_H24'    \n",
    "                )\n",
    "                compile_fit_set_weights(local_Transformer_model, global_Transformer_weights, windows_dict[cluster][client][0], client, client_names, 'Transformer')\n",
    "\n",
    "            #to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "            average_weights_LSTM = sum_scaled_weights(scaled_local_weight_LSTM_list)\n",
    "            average_weights_CNN = sum_scaled_weights(scaled_local_weight_CNN_list)\n",
    "            average_weights_Transformer = sum_scaled_weights(scaled_local_weight_Transformer_list)\n",
    "            #update global model \n",
    "            global_LSTM_model[idx].set_weights(average_weights_LSTM)\n",
    "            global_CNN_model[idx].set_weights(average_weights_CNN)\n",
    "            global_Transformer_model[idx].set_weights(average_weights_Transformer)\n",
    "\n",
    "    #Evaluate Results\n",
    "    forecasts_dict_LSTM_F5_H12 = {k: {} for k in range(N_CLUSTERS)}\n",
    "    forecasts_dict_CNN_F5_H12 = {k: {} for k in range(N_CLUSTERS)}\n",
    "    forecasts_dict_Transformer_F5_H12 = {k: {} for k in range(N_CLUSTERS)}\n",
    "\n",
    "    for idx, cluster in enumerate(windows_dict):\n",
    "        #Get names of clients within cluster\n",
    "        client_names = list()\n",
    "        for client in windows_dict[cluster]:\n",
    "            client_names.append(client)\n",
    "\n",
    "        for i, client in enumerate(windows_dict[cluster].keys()):\n",
    "            IPython.display.clear_output()\n",
    "            print(\"-------------Cluster----\", cluster, \"-----\", client,\"--------\", i+1, \"/\", len(client_names))\n",
    "\n",
    "            #LSTM\n",
    "            model_evaluation_test = test_model(windows_dict[cluster][client][0], global_LSTM_model[idx], client, MAX_EPOCHS)\n",
    "            #Save\n",
    "            forecasts_dict_LSTM_F5_H12[cluster][client] = {\n",
    "                'MSE':model_evaluation_test[0], 'RMSE':model_evaluation_test[1], 'MAPE':model_evaluation_test[2],\n",
    "                'MAE':model_evaluation_test[3], 'Time':((timetaken.logs[-1][1]) / (timetaken.logs[-1][0]+1)) \n",
    "            }\n",
    "            #CNN\n",
    "            model_evaluation_test = test_model(windows_dict[cluster][client][0], global_CNN_model[idx], client, MAX_EPOCHS)\n",
    "            #Save\n",
    "            forecasts_dict_CNN_F5_H12[cluster][client] = {\n",
    "                'MSE':model_evaluation_test[0], 'RMSE':model_evaluation_test[1], 'MAPE':model_evaluation_test[2],\n",
    "                'MAE':model_evaluation_test[3], 'Time':((timetaken.logs[-1][1]) / (timetaken.logs[-1][0]+1)) \n",
    "            }    \n",
    "            #Transformer\n",
    "            model_evaluation_test = test_model(windows_dict[cluster][client][0], global_Transformer_model[idx], client, MAX_EPOCHS)\n",
    "            #Save\n",
    "            forecasts_dict_Transformer_F5_H12[cluster][client] = {\n",
    "                'MSE':model_evaluation_test[0], 'RMSE':model_evaluation_test[1], 'MAPE':model_evaluation_test[2],\n",
    "                'MAE':model_evaluation_test[3], 'Time':((timetaken.logs[-1][1]) / (timetaken.logs[-1][0]+1)) \n",
    "            }\n",
    "\n",
    "    final_dict['Federated']['LSTM']['H12']['F5'] = forecasts_dict_LSTM_F5_H12\n",
    "    final_dict['Federated']['CNN']['H12']['F5'] = forecasts_dict_CNN_F5_H12\n",
    "    final_dict['Federated']['Transformer']['H12']['F5'] = forecasts_dict_Transformer_F5_H12\n",
    "\n",
    "\n",
    "    with open('Dictionaries_Results/Federated_resultsF5H12.pkl', 'wb') as f:\n",
    "        pickle.dump(final_dict, f)\n",
    " \n",
    "except Exception, e: \n",
    "    logger.error('Failed to upload to ftp: '+ str(e))\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
